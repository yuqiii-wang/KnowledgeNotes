# Attention Mechanism Further Explained

## Self-Attention vs Cross-Attention

Self-attention allows each element of a sequence to focus on other elements of the same sequence.

Cross-attention allows one sequence (e.g., a query sequence) to focus on another sequence (e.g., a key-value sequence).

