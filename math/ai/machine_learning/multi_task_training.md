# Multi-Task Training/Fine-Tuning

Due to limited hardware resources, a large model should be versatile handling a wide range of tasks, and should be trained with a variety of task data.
Multi-task data might be vastly different in sizes and formats, and they may give imbalanced losses in training.
Small loss tasks are likely under-trained for model tends to prioritize optimizing large loss task data.
